{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Writing Preprocessing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T11:13:39.366735Z",
     "start_time": "2019-11-29T11:13:39.360657Z"
    }
   },
   "outputs": [],
   "source": [
    "#logger.debug('Inside Logging Function')\n",
    "#logger.info('info Inside Logging Function')\n",
    "#logger.warning('warn Inside Logging Function')\n",
    "#logger.error('error Inside Logging Function')\n",
    "#logger.critical('critical Inside Logging Function')\n",
    "#logger.info('--------------- TESTING ENDS ---------------------- ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environmental Setup Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T09:10:58.286964Z",
     "start_time": "2019-12-03T09:10:57.282273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 19.3 from /home/alari/anaconda3/envs/ec_final/lib/python3.6/site-packages/pip (python 3.6)\r\n"
     ]
    }
   ],
   "source": [
    "!pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T11:53:07.017845Z",
     "start_time": "2019-11-29T11:53:06.458203Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *\n",
    "import os\n",
    "import pandas as pd\n",
    "import cloudpickle as cp\n",
    "from urllib.request import urlopen # or maybe use urllib2\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from time import strftime, localtime\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T11:13:39.493367Z",
     "start_time": "2019-11-29T11:13:39.463720Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def define_paths(parentpath):\n",
    "    PARENT_PATH = parentpath\n",
    "    if \"kwargs\" not in locals():\n",
    "        kwargs = {}\n",
    "    #LOG_PATH = os.path.relpath(\"logs\") relative_path = os.path.relpath(path, start) \n",
    "    LOG_PATH = os.path.join(PARENT_PATH,\"logs\")\n",
    "    kwargs = define_logger(log_dir=LOG_PATH,**kwargs)\n",
    "    raw_data_url = \"https://filedn.com/lK1VhM9GbBxVlERr9KFjD4B/ecommerce/data_on_public/full/products.csv\"\n",
    "    #################### MODEL GOES ALWAYS TOGETHER WITH SPECIFIC LEARNER AND DATASET\n",
    "    MODEL_NAME1 = \"3_features_80pc.pkl\"\n",
    "    logger.info(f\"Model 1 is defined as {MODEL_NAME1}\")\n",
    "    model_1_feature_names = [\"combined_name_description\", \"brand\", \"provider\"]\n",
    "    kwargs[\"model1.feature_names\"] = model_1_feature_names\n",
    "    logger.info(f\"Model 1 feature names are {model_1_feature_names}\")\n",
    "    LEARNER_REMOTE_PATH1 =\"https://filedn.com/lK1VhM9GbBxVlERr9KFjD4B/ecommerce/ulmfit_final_files/models/3_features_80pc.pkl\"\n",
    "    logger.info(f\"Model 1 remote path is defined as {LEARNER_REMOTE_PATH1}\")\n",
    "    MODEL_FOLDER=os.path.join(PARENT_PATH,\"models\")\n",
    "    make_sure_path_exists(MODEL_FOLDER)\n",
    "    logger.info(f\"Model folder is defined as {MODEL_FOLDER}\")\n",
    "    LOCAL_MODEL_FULL_PATH = os.path.join(MODEL_FOLDER,MODEL_NAME)\n",
    "    BASE_DATA_PATH = os.path.join(PARENT_PATH,\"data\")\n",
    "    BASE_DATA_FILE = os.path.join(BASE_DATA_PATH,\"products.csv\")\n",
    "    make_sure_path_exists(BASE_DATA_PATH)\n",
    "    kwargs[\"base_data_path\"] = BASE_DATA_PATH\n",
    "    kwargs[\"fulldata\"] = raw_data_url\n",
    "    logger.info(f\"Raw data remote path is defined as {raw_data_url}\")\n",
    "    kwargs[\"model1.3_features\"]=LEARNER_REMOTE_PATH1\n",
    "    kwargs[\"logger\"] = logger\n",
    "    kwargs[\"base_data_file\"] = BASE_DATA_FILE\n",
    "   # base_data_path = os.path.abspath(os.path.join('data'))\n",
    "    #kwargs_paths[\"base_data_path\"] = base_data_path\n",
    "    return kwargs\n",
    "\n",
    "\n",
    "def define_logger(log_dir,log_to_this_file=\"e_commerce_logger\",log_to_file=False,**kwargs):\n",
    "    if \"kwargs\" not in locals():\n",
    "        kwargs = {}\n",
    "    make_sure_path_exists(log_dir)\n",
    "    log_to_this_file = os.path.join(log_dir, log_to_this_file)\n",
    "    log_to_this_file = log_to_this_file + strftime(\"%Y_%m_%d_%H_%M\", localtime()) + \".log\"\n",
    "    logger = logging.getLogger('e_commerce_project_logger')\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    # create file handler which logs even debug messages\n",
    "    if log_to_file == True:\n",
    "        fh = logging.FileHandler(log_to_this_file)\n",
    "        fh.setLevel(logging.DEBUG)\n",
    "        # create console handler with a higher log level\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        fh.setFormatter(formatter)\n",
    "        logger.addHandler(fh)\n",
    "        print(f\"Logging all updates to {log_to_this_file} this file\")\n",
    "        kwargs[\"logger_path\"] = log_to_this_file\n",
    "    kwargs[\"logger\"] = logger\n",
    "    return kwargs\n",
    "\n",
    "def download_from_url(local_path,remote_path,mode=\"data\",**kwargs): \n",
    "    logger = kwargs.get(\"logger\")\n",
    "    logger.info(f\"Starting to download {mode} from {remote_path} remote resource\")\n",
    "    if mode == \"data\":\n",
    "        FILENAME = \"products.csv\"\n",
    "        folder_to_check = kwargs[\"base_data_path\"]\n",
    "                \n",
    "    elif mode == \"model\":\n",
    "        FILENAME = MODEL_NAME\n",
    "        folder_to_check = MODEL_FOLDER\n",
    "        \n",
    "    if FILENAME not in os.listdir(folder_to_check):\n",
    "        logger.debug(f\"It seems the {mode} hasn't been downloaded yet, so we will do so now: \\n\")\n",
    "        logger.debug(f\"=================DOWNLOADING THE {mode} ==================\")\n",
    "        filedata = urlopen(remote_path)\n",
    "        datatowrite = filedata.read()\n",
    "        with open(local_path, 'wb') as f:\n",
    "            f.write(datatowrite)\n",
    "\n",
    "        if FILENAME not in os.listdir(folder_to_check):\n",
    "            logger.warn(f\"It seems {mode} download didn't work, please check the function!\")\n",
    "                \n",
    "    else:\n",
    "        logger.info(f\"{mode} is present in {folder_to_check}, proceed normally!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T11:13:39.531292Z",
     "start_time": "2019-11-29T11:13:39.495521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switched working directory to /home/alari/pCloudDrive/Public Folder/ecommerce/e_commerce_hierarchical/e_commerce_project/code/using_predictions/submission/runtime, STARTING PROGRAM\n"
     ]
    }
   ],
   "source": [
    "PARENT_PATH = \"/home/alari/pCloudDrive/Public Folder/ecommerce/e_commerce_hierarchical/e_commerce_project/code/using_predictions/submission/runtime\"\n",
    "make_sure_path_exists(PARENT_PATH)\n",
    "os.chdir(PARENT_PATH)  # #os.path.pardir,\n",
    "print(f\"Switched working directory to {PARENT_PATH}, STARTING PROGRAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test to See that Data Downloading Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T11:13:39.568585Z",
     "start_time": "2019-11-29T11:13:39.565617Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#download_from_url(mode=\"data\",local_path=kwargs.get(\"base_data_file\"),remote_path = kwargs.get(\"fulldata\"),**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T11:13:39.592270Z",
     "start_time": "2019-11-29T11:13:39.578595Z"
    }
   },
   "outputs": [],
   "source": [
    "# test with this smaller link https://filedn.com/lK1VhM9GbBxVlERr9KFjD4B/ecommerce/ulmfit_final_files/data/val.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING FUNCTIONS DEFINED HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T12:56:09.205603Z",
     "start_time": "2019-11-29T12:56:09.152441Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace_missing_cols_with_empty(df,cols):\n",
    "    df.loc[:, cols] = df.loc[:, cols].replace(np.nan, '', regex=True)\n",
    "    return df\n",
    "\n",
    "def drop_if_exists(df, cols):\n",
    "\tfor col in cols:\n",
    "\t\tif col in df.columns:\n",
    "\t\t\tdf = df.drop(col, axis=1)\n",
    "\treturn df\n",
    "\n",
    "def text_cleaner(text):\n",
    "\trules = [\n",
    "\t\t{r'>\\s+': u'>'},  # remove spaces after a tag opens or closes\n",
    "\t\t{r'\\s+': u' '},  # replace consecutive spaces\n",
    "\t\t{r'\\s*<br\\s*/?>\\s*': u'\\n'},  # newline after a <br>\n",
    "\t\t{r'</(div)\\s*>\\s*': u'\\n'},  # newline after </p> and </div> and <h1/>...\n",
    "\t\t{r'</(p|h\\d)\\s*>\\s*': u'\\n\\n'},  # newline after </p> and </div> and <h1/>...\n",
    "\t\t{r'<head>.*<\\s*(/head|body)[^>]*>': u''},  # remove <head> to </head>\n",
    "\t\t{r'<a\\s+href=\"([^\"]+)\"[^>]*>.*</a>': r'\\1'},  # show links instead of texts\n",
    "\t\t{r'[ \\t]*<[^<]*?/?>': u''},  # remove remaining tags\n",
    "\t\t{r'^\\s+': u''} # remove spaces at the beginning\n",
    "    ]\n",
    "\tfor rule in rules:\n",
    "\t\tfor (k, v) in rule.items():\n",
    "\t\t\tregex = re.compile(k)\n",
    "\t\t\ttext = regex.sub(v, text)\n",
    "\ttext = text.rstrip()\n",
    "\treturn text.lower()\n",
    "\n",
    "def savePickle(data, picklefile):\n",
    "\twith open(picklefile, \"wb\") as f:\n",
    "\t\tpickle.dump(data, f)\n",
    "\tf.close()\n",
    "    \n",
    "def step_1_data_processing(df,**kwargs):\n",
    "    logger = kwargs.get(\"logger\")\n",
    "    \n",
    "    \n",
    "    initial_feature_names =  kwargs[\"initial_feature_names\"]\n",
    "    logger.info(f\"Feature names of un-processed raw data are {initial_feature_names} \\n\")\n",
    "  \n",
    "\n",
    "    df[\"cleaned_name\"] = df.progress_apply(lambda row: text_cleaner(str(row[\"name\"]).lower()), axis=1)\n",
    "    df[\"cleaned_description\"] = df.progress_apply(lambda row: text_cleaner(str(row[\"description\"]).lower()), axis=1)\n",
    "    df[\"combined_name_description\"] = df.progress_apply(\n",
    "        lambda row: str(row[\"cleaned_name\"]).lower() + str(row[\"cleaned_description\"]).lower(), axis=1)\n",
    "    \n",
    "    model1_feature_names =  kwargs[\"model1.feature_names\"]\n",
    "    logger.info(f\"Retaining only {model1_feature_names} out of all features. Also replacing NAN with empty.\")\n",
    "\n",
    "    df = replace_missing_cols_with_empty(df,cols=model1_feature_names)\n",
    "    target_with_feature_names = [i for i in itertools.chain(model1_feature_names, [\"mapped_id\"])]\n",
    "    df = df.loc[:,target_with_feature_names]\n",
    "    logger.info(\"Category preprocessing function finished!\")\n",
    "    return df\n",
    "\n",
    "def step_2_save_splits_from_dataframe(df,feature_names=[\"combined_name_description\", \"brand\", \"provider\", \"summary\"], \\\n",
    "                                             target_name=[\"mapped_id\"],**kwargs):\n",
    "    save_splits_again = kwargs.get(\"save_splits_again\")\n",
    "    logger = kwargs.get(\"logger\")\n",
    "    df.loc[:, target_name] = df.loc[:, target_name].astype(int)\n",
    "    df = df.groupby(target_name).filter(lambda x: len(x) > 1)\n",
    "    logger.info(f\"Target classes with only 1 count have been filtered out. Resulting dataframe has {df.shape[0]} elements\")\n",
    "    features = df.loc[:, feature_names].reset_index(drop=True)\n",
    "    target = df.loc[:,target_name].reset_index(drop=True)\n",
    "    time_now = strftime(\"%Y_%m_%d_%H_%M\", localtime())\n",
    "    logger.info(f\"Time now is {time_now}\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(features, target, test_size=0.20,\n",
    "                                                      random_state=42, stratify=target)\n",
    "\n",
    "    ml_splits = {'X_train': X_train, 'X_val': X_val, 'y_train': y_train, 'y_val': y_val}\n",
    "\n",
    "    # Here we are going to create the output dictionary to save the machine learning splits in one Pickle file\n",
    "    train_val_dict = {}\n",
    "    train_val_dict[\"train\"] = pd.concat([ml_splits.get(\"X_train\"),ml_splits.get(\"y_train\")],axis=1)\n",
    "    train_val_dict[\"val\"] = pd.concat([ml_splits.get(\"X_val\"),ml_splits.get(\"y_val\")],axis=1)\n",
    "    logger.info(\"Header of the train-validation machine learning splits dictionary look like this: \\n\")\n",
    "    logger.info(str(apply_fun(train_val_dict,fun=pd.DataFrame.head)))\n",
    "    #ml_splits_wout_summary_nastat = ut.apply_fun(dict_=ml_splits_wout_summary,fun=pd.DataFrame.isna)\n",
    "    #na_statistics = ut.apply_fun(dict_=ml_splits_wout_summary_nastat,fun=pd.DataFrame.sum)\n",
    "    #logger.info(f\"The NA Statistics look the following {str(na_statistics)}\")\n",
    "    if save_splits_again == True:\n",
    "        time_now = strftime(\"%Y_%m_%d_%H_%M\", localtime())\n",
    "        splits_end = f\"ml_splits_{time_now}.pkl\"\n",
    "        splits_path = os.path.join(kwargs_paths.get(\"base_data_path\"), splits_end)\n",
    "        savePickle(data=training_dict_save, picklefile=splits_path)\n",
    "        logger.info(f\"Saving ML Splits to the path {splits_path}\")\n",
    "    return train_val_dict\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTION FUNCTIONS DEFINED HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T12:59:13.135344Z",
     "start_time": "2019-11-29T12:59:13.120386Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_fun(dict_: dict, fun, **kwargs):\n",
    "\ttransformed_dict = dict((k, fun(v, **kwargs)) for k, v in dict_.items())\n",
    "\treturn transformed_dict\n",
    "\n",
    "def select_features_from_data(data,feature_names=[\"recombined_name_description\", \"brand\", \"provider\"]):\n",
    "    features = data.loc[:, feature_names].reset_index(drop=True)\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_one_prediction(feature_row):\n",
    "    one_prediction_unformatted = model.predict(feature_row)[0]\n",
    "    print(one_prediction_unformatted)\n",
    "    one_prediction_int = int(one_prediction_unformatted)\n",
    "    return one_prediction_int\n",
    "\n",
    "\n",
    "\n",
    "def obtain_predictions_in_loop(features):\n",
    "        for i,feature_row in tqdm(features.iterrows(),total=features.shape[0]):\n",
    "        #for i,row in (data_full.iterrows()):\n",
    "            prediction= get_one_prediction(feature_row)\n",
    "            print(f\"Feature row {i} looks like this: \\n\")\n",
    "            print(f\"{str(feature_row.values)}\\n\")\n",
    "            print(f\"The corresponding prediction row {i} is: \\n\")\n",
    "            print(prediction)\n",
    "            if i==10:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T11:13:39.876120Z",
     "start_time": "2019-11-29T11:13:39.849456Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#import logging\n",
    "#logger = logging.getLogger()\n",
    "#logger.setLevel(logging.DEBUG)\n",
    "#logging.debug(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Main Program Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:02:22.974532Z",
     "start_time": "2019-11-29T12:59:22.125733Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/111363 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3_features_80pc.pkl loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 1/111363 [00:00<12:03:01,  2.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n",
      "Feature row 0 looks like this: \n",
      "\n",
      "combined_name_description    neulepusero roxy crazy cityneulepusero roxy cr...\n",
      "brand                                                                     Roxy\n",
      "provider                                                               Spartoo\n",
      "Name: 0, dtype: object\n",
      "\n",
      "The corresponding prediction row 0 is: \n",
      "\n",
      "71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 2/111363 [00:00<11:20:37,  2.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3553\n",
      "Feature row 1 looks like this: \n",
      "\n",
      "combined_name_description    swedish grace lautanen 21 cm midnattnan\n",
      "brand                                                      Rörstrand\n",
      "provider                                                 KitchenTime\n",
      "Name: 1, dtype: object\n",
      "\n",
      "The corresponding prediction row 1 is: \n",
      "\n",
      "737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 3/111363 [00:01<13:28:38,  2.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "Feature row 2 looks like this: \n",
      "\n",
      "combined_name_description    emporio armani connected miesten kello art3022...\n",
      "brand                                                           Emporio Armani\n",
      "provider                                                           Laatukellot\n",
      "Name: 2, dtype: object\n",
      "\n",
      "The corresponding prediction row 2 is: \n",
      "\n",
      "64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 4/111363 [00:02<15:58:19,  1.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631\n",
      "Feature row 3 looks like this: \n",
      "\n",
      "combined_name_description    ferm living hook ripustin, iso, teräs - sinine...\n",
      "brand                                                              Ferm Living\n",
      "provider                                                   Finnish Design Shop\n",
      "Name: 3, dtype: object\n",
      "\n",
      "The corresponding prediction row 3 is: \n",
      "\n",
      "238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 5/111363 [00:02<14:48:29,  2.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1604\n",
      "Feature row 4 looks like this: \n",
      "\n",
      "combined_name_description    san jose distressed frayed high waist skinny j...\n",
      "brand                                                                         \n",
      "provider                                                          Fiorellashop\n",
      "Name: 4, dtype: object\n",
      "\n",
      "The corresponding prediction row 4 is: \n",
      "\n",
      "440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 6/111363 [00:02<14:34:42,  2.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n",
      "Feature row 5 looks like this: \n",
      "\n",
      "combined_name_description    asweego cc shoes sport shoes running shoes vih...\n",
      "brand                                                       adidas Performance\n",
      "provider                                                                 Boozt\n",
      "Name: 5, dtype: object\n",
      "\n",
      "The corresponding prediction row 5 is: \n",
      "\n",
      "55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 7/111363 [00:03<20:01:19,  1.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2441\n",
      "Feature row 6 looks like this: \n",
      "\n",
      "combined_name_description    yrttihoitoainehiuksia hellivä hoitoaine sopii ...\n",
      "brand                                                                Frantsila\n",
      "provider                                                                 Ekolo\n",
      "Name: 6, dtype: object\n",
      "\n",
      "The corresponding prediction row 6 is: \n",
      "\n",
      "519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 8/111363 [00:04<17:15:32,  1.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n",
      "Feature row 7 looks like this: \n",
      "\n",
      "combined_name_description    kengät asics gel-lyte iiikengät asics gel-lyte...\n",
      "brand                                                                    Asics\n",
      "provider                                                               Spartoo\n",
      "Name: 7, dtype: object\n",
      "\n",
      "The corresponding prediction row 7 is: \n",
      "\n",
      "55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 9/111363 [00:04<15:26:41,  2.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n",
      "Feature row 8 looks like this: \n",
      "\n",
      "combined_name_description    sandaalit el naturalista leavessandaalit el na...\n",
      "brand                                                           El Naturalista\n",
      "provider                                                               Spartoo\n",
      "Name: 8, dtype: object\n",
      "\n",
      "The corresponding prediction row 8 is: \n",
      "\n",
      "55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 10/111363 [00:04<14:02:22,  2.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1604\n",
      "Feature row 9 looks like this: \n",
      "\n",
      "combined_name_description    get farkkutakki denimtakki musta tiger of swed...\n",
      "brand                                                    Tiger of Sweden Jeans\n",
      "provider                                                                 Boozt\n",
      "Name: 9, dtype: object\n",
      "\n",
      "The corresponding prediction row 9 is: \n",
      "\n",
      "440\n",
      "203\n",
      "Feature row 10 looks like this: \n",
      "\n",
      "combined_name_description    hooded water-repellent coat toppatakki harmaa ...\n",
      "brand                                                               Mango Kids\n",
      "provider                                                                 Boozt\n",
      "Name: 10, dtype: object\n",
      "\n",
      "The corresponding prediction row 10 is: \n",
      "\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "kwargs = define_paths(parentpath=PARENT_PATH)\n",
    "logger=kwargs.get(\"logger\")\n",
    "kwargs[\"parentpath\"] = PARENT_PATH # parentpath is the project path from which the code is run\n",
    "kwargs[\"skip\"] = \"\"#\"0 2 \" --- data pipeline is organized as sequential steps. Write here the steps you wish to skip\n",
    "kwargs[\"amountlines\"]=int(-1) # -1 corresponds to all lines of raw data read in from CSV, otherwise the integer specifies how many lines are are read in from beginning\n",
    "kwargs[\"save_splits_again\"] = False # whether to re-save the machine learning train-test split file again\n",
    "\n",
    "\n",
    "def main(**kwargs):\n",
    "    \"\"\"This is the caller function of the data pipeline, producing predictions of E-Commerce Categories from a defined set of features.\"\"\"\n",
    "    step1=step2={}\n",
    "    # data_object is the dictionary containing outputs of data pipeline steps\n",
    "    data_object = {\"step1\":step1,\"step2\":step2} \n",
    "    logger = kwargs.get(\"logger\")\n",
    "    data_object[\"kwargs_main\"] = kwargs\n",
    "    #print(main.__doc__)\n",
    "    #parser = argparse.ArgumentParser()\n",
    "    #parser.add_argument(\"-skip\",\"--skip-steps\", default=\"2\", type=str,help=\"Steps to skip separated by space\")\n",
    "    #args = parser.parse_args()\n",
    "    skip_steps = kwargs.get(\"skip\").split(' ') #     df = kwargs.get(\"raw_data\")\n",
    "\n",
    "    logger.info(\" ----------- STEP 1 : DATA LOADING AND PROCESSING  --------------\")\n",
    "\n",
    "    if \"data_read\" not in skip_steps:\n",
    "        # model1.3_features\n",
    "        download_from_url(mode=\"data\",local_path=kwargs.get(\"base_data_file\"),remote_path = kwargs.get(\"fulldata\"),**kwargs)\n",
    "        raw_data_local_path = kwargs.get(\"base_data_file\")\n",
    "        print(f\"THE RAW DATA LOCAL PATH FROM KWARGS IS {raw_data_local_path}\")\n",
    "        amountlines = kwargs.get(\"amountlines\")\n",
    "        if amountlines != -1:\n",
    "            rows_to_keep = list(range(amountlines))\n",
    "            df = pd.read_csv(raw_data_local_path, skiprows=lambda x: x not in rows_to_keep)\n",
    "            logger.info(f\"----- READING {rows_to_keep} lines of  DATA FROM  path {raw_data_path} COMPLETED ------------------ \")\n",
    "        else:\n",
    "            df = pd.read_csv(raw_data_local_path)\n",
    "            logger.info(f\"----- READING FULL DATA FROM  path {raw_data_local_path} COMPLETED ------------------ \")\n",
    "    \n",
    "    cols_to_drop = ['ean', 'url', 'price', './._prods.csv']\n",
    "    logger.info(f\"Going to drop these columns if they exist: {cols_to_drop}\\n\")\n",
    "    df = drop_if_exists(df, cols=cols_to_drop)\n",
    "    initial_feature_names = [x for x in df.columns.tolist() if x!=\"mapped_id\"]\n",
    "    kwargs[\"initial_feature_names\"] = initial_feature_names\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"THERE IS NO DATA TO BE READ, PROGRAM STOPPING!\")\n",
    "        return\n",
    "\n",
    "    tqdm.pandas(desc='PROGRESS>>>')\n",
    "    data_1 = step_1_data_processing(df,**kwargs)\n",
    "\n",
    "    step1[\"data\"] = data_1\n",
    "    data_object[\"data_read\"] = step1\n",
    "    logger.info(str(data_1.head()))\n",
    "    logger.info(\" ----------- STEP 1 COMPLETE : DATA LOADING AND PROCESSING  --------------\")\n",
    "\n",
    "    logger.info(\" ----------- STEP 2 BEGINS : DATA PRE-PROCESSING  --------------\")\n",
    "\n",
    "    \"\"\" Preferred mode : read data from disk\"\"\"\n",
    "    if \"data_process\" not in skip_steps:\n",
    "        data_2 = step_2_save_splits_from_dataframe(df=data_1,feature_names=[\"combined_name_description\", \"brand\", \"provider\"],**kwargs)\n",
    "    else:\n",
    "        data_2 = []\n",
    "        #data_2 = read_data_splits_pickle_from_cloud(pickle_url=\"https://filedn.com/lK1VhM9GbBxVlERr9KFjD4B/ecommerce/ulmfit_final_files/data/final_splits.pkl\")\n",
    "        #DATA_to_validate = DATA_splits[\"val\"]\n",
    "    step2[\"data\"] = data_2\n",
    "    data_object[\"data_process\"] = step2\n",
    "\n",
    "\n",
    "    logger.info(f\" ----------- STEP 2, preprocessing, ENDED.  --------------\")\n",
    "\n",
    "    logger.info(\" ----------- STEP 3 : DOWNLOADING AND LOADING PRETRAINED HIGH DIMENSIONAL CLASSIFICATION MODEL  --------------\")\n",
    "    if \"model\" not in skip_steps:\n",
    "        model =  download_and_load_model()\n",
    "    data_object[\"model\"] = model \n",
    "\n",
    "    if \"predictions\" not in skip_steps:\n",
    "        DATA_to_validate = data_2[\"val\"]\n",
    "        FEATURES_to_validate = select_features_from_data(data=DATA_to_validate,feature_names=[\"combined_name_description\", \"brand\", \"provider\"])\n",
    "        # FEATURES_to_validate.reset_index(drop=True)\n",
    "        obtain_predictions_in_loop(features=FEATURES_to_validate)\n",
    "        data_object[\"features\"] = FEATURES_to_validate\n",
    "\n",
    "    #data_object[\"predictions\"] = predictions # save the predictions here somewhere\n",
    "    return data_object\n",
    "\n",
    "\n",
    "data_pipeline_object = main(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T13:05:37.169457Z",
     "start_time": "2019-11-29T13:05:37.157088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (0 items)\n",
       "x: TextList\n",
       "\n",
       "y: CategoryList\n",
       "\n",
       "Path: /home/alari/pCloudDrive/Public Folder/ecommerce/e_commerce_hierarchical/e_commerce_project/code/using_predictions/submission/models;\n",
       "\n",
       "Valid: LabelList (0 items)\n",
       "x: TextList\n",
       "\n",
       "y: CategoryList\n",
       "\n",
       "Path: /home/alari/pCloudDrive/Public Folder/ecommerce/e_commerce_hierarchical/e_commerce_project/code/using_predictions/submission/models;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(60000, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1150, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1150, 1150, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1150, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.06999999999999999, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=1313, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f043965fd90>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/alari/pCloudDrive/Public Folder/ecommerce/e_commerce_hierarchical/e_commerce_project/code/using_predictions/submission/models'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (0 items)\n",
       "x: TextList\n",
       "\n",
       "y: CategoryList\n",
       "\n",
       "Path: /home/alari/pCloudDrive/Public Folder/ecommerce/e_commerce_hierarchical/e_commerce_project/code/using_predictions/submission/models;\n",
       "\n",
       "Valid: LabelList (0 items)\n",
       "x: TextList\n",
       "\n",
       "y: CategoryList\n",
       "\n",
       "Path: /home/alari/pCloudDrive/Public Folder/ecommerce/e_commerce_hierarchical/e_commerce_project/code/using_predictions/submission/models;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(60000, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1150, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1150, 1150, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1150, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.06999999999999999, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=1313, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f043965fd90>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/alari/pCloudDrive/Public Folder/ecommerce/e_commerce_hierarchical/e_commerce_project/code/using_predictions/submission/models'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): Embedding(60000, 400, padding_idx=1)\n",
       "  (1): Embedding(60000, 400, padding_idx=1)\n",
       "  (2): LSTM(400, 1150, batch_first=True)\n",
       "  (3): LSTM(1150, 1150, batch_first=True)\n",
       "  (4): LSTM(1150, 400, batch_first=True)\n",
       "  (5): RNNDropout()\n",
       "  (6): RNNDropout()\n",
       "  (7): RNNDropout()\n",
       "  (8): RNNDropout()\n",
       "  (9): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): Dropout(p=0.06999999999999999, inplace=False)\n",
       "  (11): Linear(in_features=1200, out_features=50, bias=True)\n",
       "  (12): ReLU(inplace=True)\n",
       "  (13): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): Dropout(p=0.1, inplace=False)\n",
       "  (15): Linear(in_features=50, out_features=1313, bias=True)\n",
       ")], add_time=True, silent=False)\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(60000, 400, padding_idx=1)\n",
       "  (1): Embedding(60000, 400, padding_idx=1)\n",
       "  (2): LSTM(400, 1150, batch_first=True)\n",
       "  (3): LSTM(1150, 1150, batch_first=True)\n",
       "  (4): LSTM(1150, 400, batch_first=True)\n",
       "  (5): RNNDropout()\n",
       "  (6): RNNDropout()\n",
       "  (7): RNNDropout()\n",
       "  (8): RNNDropout()\n",
       "  (9): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): Dropout(p=0.06999999999999999, inplace=False)\n",
       "  (11): Linear(in_features=1200, out_features=50, bias=True)\n",
       "  (12): ReLU(inplace=True)\n",
       "  (13): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): Dropout(p=0.1, inplace=False)\n",
       "  (15): Linear(in_features=50, out_features=1313, bias=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = data_pipeline_object.get(\"model\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Predictions : Step-by-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T04:11:07.695371Z",
     "start_time": "2019-11-29T04:11:07.672713Z"
    }
   },
   "source": [
    "## Preprocessing the Data for Loading it Into the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T10:18:40.781322Z",
     "start_time": "2019-11-29T10:18:40.751774Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_FOLDER=os.path.abspath(\"models\")\n",
    "\n",
    "def make_sure_path_exists(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        try:\n",
    "            os.makedirs(dir)\n",
    "            print(f\"{dir} path didn't exist, created! Proceed now!\")\n",
    "        except OSError as exception:\n",
    "                print(\"Directory Creation Exception\")\n",
    "\n",
    "\n",
    "        \n",
    "make_sure_path_exists(MODEL_FOLDER)\n",
    "LEARNER_REMOTE_PATH =\"https://filedn.com/lK1VhM9GbBxVlERr9KFjD4B/ecommerce/ulmfit_final_files/models/3_features_80pc.pkl\"\n",
    "MODEL_NAME = \"3_features_80pc.pkl\"\n",
    "LOCAL_MODEL_FULL_PATH = os.path.join(MODEL_FOLDER,MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Data to Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T05:02:58.816109Z",
     "start_time": "2019-11-29T05:02:58.807648Z"
    }
   },
   "outputs": [],
   "source": [
    "############ Define DATA and FEATURES in single CELL ##################\n",
    "def read_data_splits_pickle_from_cloud(pickle_url):\n",
    "    data = cp.load(urlopen(pickle_url))\n",
    "    return data\n",
    "\n",
    "DATA_splits = read_data_splits_pickle_from_cloud(pickle_url=\"https://filedn.com/lK1VhM9GbBxVlERr9KFjD4B/ecommerce/ulmfit_final_files/data/final_splits.pkl\")\n",
    "DATA_to_validate = DATA_splits[\"val\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are the Cells To be Executed One by One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T04:12:58.436899Z",
     "start_time": "2019-11-29T04:12:58.412738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alari/pCloudDrive/Public Folder/ecommerce/e_commerce_hierarchical/e_commerce_project/code/using_predictions/submission/models path exists, proceed!\n"
     ]
    }
   ],
   "source": [
    "MODEL_FOLDER=os.path.abspath(\"models\")\n",
    "\n",
    "        \n",
    "make_sure_path_exists(MODEL_FOLDER)\n",
    "LEARNER_REMOTE_PATH =\"https://filedn.com/lK1VhM9GbBxVlERr9KFjD4B/ecommerce/ulmfit_final_files/models/3_features_80pc.pkl\"\n",
    "MODEL_NAME = \"3_features_80pc.pkl\"\n",
    "LOCAL_MODEL_FULL_PATH = os.path.join(MODEL_FOLDER,MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T04:44:59.212707Z",
     "start_time": "2019-11-29T04:44:58.432597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3_features_80pc.pkl loaded!\n"
     ]
    }
   ],
   "source": [
    "# In this cell, we are going to download and load the model for making predictions later\n",
    "\n",
    "\n",
    " \n",
    "def load_model_from_local_path(model_folder=MODEL_FOLDER,model_name=MODEL_NAME):\n",
    "    model = load_learner(path=model_folder,\n",
    "                     file=model_name,**{\"num_workers\":0}) # OR NUM WORKERS\n",
    "    if \"model\" in locals():\n",
    "        print(f\"Model {model_name} loaded!\")\n",
    "    else :\n",
    "        print(\"Something went wrong loading the model\")\n",
    "    return model\n",
    "\n",
    "def download_and_load_model():\n",
    "    download_from_url(mode=\"model\",local_path=LOCAL_MODEL_FULL_PATH,remote_path=LEARNER_REMOTE_PATH)\n",
    "    model = load_model_from_local_path()\n",
    "    return model\n",
    "\n",
    "model = download_and_load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T05:14:13.282733Z",
     "start_time": "2019-11-29T05:14:08.008385Z"
    },
    "code_folding": [
     14
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/111363 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 1/111363 [00:00<11:00:47,  2.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n",
      "Feature row 0 looks like this: \n",
      "\n",
      "combined_name_description    neulepusero roxy crazy cityneulepusero roxy cr...\n",
      "brand                                                                     Roxy\n",
      "provider                                                               Spartoo\n",
      "Name: 0, dtype: object\n",
      "\n",
      "The corresponding prediction row 0 is: \n",
      "\n",
      "71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 2/111363 [00:00<10:55:35,  2.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3553\n",
      "Feature row 1 looks like this: \n",
      "\n",
      "combined_name_description    swedish grace lautanen 21 cm midnatt\n",
      "brand                                                   Rörstrand\n",
      "provider                                              KitchenTime\n",
      "Name: 1, dtype: object\n",
      "\n",
      "The corresponding prediction row 1 is: \n",
      "\n",
      "737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 3/111363 [00:01<13:55:57,  2.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "Feature row 2 looks like this: \n",
      "\n",
      "combined_name_description    emporio armani connected miesten kello art3022...\n",
      "brand                                                           Emporio Armani\n",
      "provider                                                           Laatukellot\n",
      "Name: 2, dtype: object\n",
      "\n",
      "The corresponding prediction row 2 is: \n",
      "\n",
      "64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 4/111363 [00:02<16:26:08,  1.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631\n",
      "Feature row 3 looks like this: \n",
      "\n",
      "combined_name_description    ferm living hook ripustin, iso, teräs - sinine...\n",
      "brand                                                              Ferm Living\n",
      "provider                                                   Finnish Design Shop\n",
      "Name: 3, dtype: object\n",
      "\n",
      "The corresponding prediction row 3 is: \n",
      "\n",
      "238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 5/111363 [00:02<14:37:03,  2.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1604\n",
      "Feature row 4 looks like this: \n",
      "\n",
      "combined_name_description    san jose distressed frayed high waist skinny j...\n",
      "brand                                                                         \n",
      "provider                                                          Fiorellashop\n",
      "Name: 4, dtype: object\n",
      "\n",
      "The corresponding prediction row 4 is: \n",
      "\n",
      "440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 6/111363 [00:02<14:23:15,  2.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n",
      "Feature row 5 looks like this: \n",
      "\n",
      "combined_name_description    asweego cc shoes sport shoes running shoes vih...\n",
      "brand                                                       adidas Performance\n",
      "provider                                                                 Boozt\n",
      "Name: 5, dtype: object\n",
      "\n",
      "The corresponding prediction row 5 is: \n",
      "\n",
      "55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 7/111363 [00:03<19:43:53,  1.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2441\n",
      "Feature row 6 looks like this: \n",
      "\n",
      "combined_name_description    yrttihoitoainehiuksia hellivä hoitoaine sopii ...\n",
      "brand                                                                Frantsila\n",
      "provider                                                                 Ekolo\n",
      "Name: 6, dtype: object\n",
      "\n",
      "The corresponding prediction row 6 is: \n",
      "\n",
      "519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 8/111363 [00:04<16:26:01,  1.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n",
      "Feature row 7 looks like this: \n",
      "\n",
      "combined_name_description    kengät asics gel-lyte iiikengät asics gel-lyte...\n",
      "brand                                                                    Asics\n",
      "provider                                                               Spartoo\n",
      "Name: 7, dtype: object\n",
      "\n",
      "The corresponding prediction row 7 is: \n",
      "\n",
      "55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 9/111363 [00:04<14:11:22,  2.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n",
      "Feature row 8 looks like this: \n",
      "\n",
      "combined_name_description    sandaalit el naturalista leavessandaalit el na...\n",
      "brand                                                           El Naturalista\n",
      "provider                                                               Spartoo\n",
      "Name: 8, dtype: object\n",
      "\n",
      "The corresponding prediction row 8 is: \n",
      "\n",
      "55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 10/111363 [00:04<12:51:32,  2.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1604\n",
      "Feature row 9 looks like this: \n",
      "\n",
      "combined_name_description    get farkkutakki denimtakki musta tiger of swed...\n",
      "brand                                                    Tiger of Sweden Jeans\n",
      "provider                                                                 Boozt\n",
      "Name: 9, dtype: object\n",
      "\n",
      "The corresponding prediction row 9 is: \n",
      "\n",
      "440\n",
      "203\n",
      "Feature row 10 looks like this: \n",
      "\n",
      "combined_name_description    hooded water-repellent coat toppatakki harmaa ...\n",
      "brand                                                               Mango Kids\n",
      "provider                                                                 Boozt\n",
      "Name: 10, dtype: object\n",
      "\n",
      "The corresponding prediction row 10 is: \n",
      "\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "def select_features_from_data(data,feature_names=[\"combined_name_description\", \"brand\", \"provider\"]):\n",
    "    features = data.loc[:, feature_names].reset_index(drop=True)\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_one_prediction(feature_row):\n",
    "    one_prediction_unformatted = model.predict(feature_row)[0]\n",
    "    print(one_prediction_unformatted)\n",
    "    one_prediction_int = int(one_prediction_unformatted)\n",
    "    return one_prediction_int\n",
    "\n",
    "\n",
    "\n",
    "def obtain_predictions_in_loop(features):\n",
    "        for i,feature_row in tqdm(features.iterrows(),total=features.shape[0]):\n",
    "        #for i,row in (data_full.iterrows()):\n",
    "            prediction= get_one_prediction(feature_row)\n",
    "            print(f\"Feature row {i} looks like this: \\n\")\n",
    "            print(f\"{str(feature_row.values)}\\n\")\n",
    "            print(f\"The corresponding prediction row {i} is: \\n\")\n",
    "            print(prediction)\n",
    "            if i==10:\n",
    "                break\n",
    "            \n",
    "FEATURES = select_features_from_data(data=DATA_to_validate)\n",
    "obtain_predictions_in_loop(features=FEATURES)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "638.496px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "555.498px",
    "left": "1365px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
